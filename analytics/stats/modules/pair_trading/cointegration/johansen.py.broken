"""
Johansen Cointegration Test Implementation for Pair Trading.
"""

import numpy as np
import pandas as pd
from scipy.stats import chi2
from typing import Dict, Tuple, Optional
from ..base_pair_statistic import BasePairStatistic


class JohansenCointegration(BasePairStatistic):
    """
    Implements the Johansen cointegration test for detecting
    long-run equilibrium relationships between price series.
    """
      # Class attributes required by the orchestrator
    name = "johansen_cointegration"
    description = "Johansen cointegration test for pair trading"
    category = "statistical"
    required_columns = ["close"]
    
    def __init__(self):
        super().__init__()
        # Critical values for Johansen test (5% significance level)
        # These are approximate values for the trace statistic
        self.critical_values = {
            'r=0': 15.41,  # No cointegration
            'r<=1': 3.76   # At most 1 cointegrating relationship
        }
      def calculate(self, data1: pd.DataFrame, data2: pd.DataFrame, 
                 **kwargs) -> Dict:
        """
        Calculate Johansen cointegration test statistics.
        
        Args:
            data1: Price data for first asset
            data2: Price data for second asset
            **kwargs: Additional parameters
            
        Returns:
            Dictionary containing cointegration test results
        """
        try:
            # Extract close prices
            symbol1_data = data1['close'] if 'close' in data1.columns else data1.iloc[:, 0]
            symbol2_data = data2['close'] if 'close' in data2.columns else data2.iloc[:, 0]
            # Align the data
            aligned_data = pd.DataFrame({
                'price1': symbol1_data,
                'price2': symbol2_data
            }).dropna()
            
            if len(aligned_data) < 30:
                return self._error_result("Insufficient data points")
            
            # Use log prices for cointegration analysis
            log_prices = np.log(aligned_data.values)
            
            # Perform Johansen test
            result = self._johansen_test(log_prices)
            
            return {
                'success': True,
                'trace_statistic': result['trace_stat'],
                'max_eigenvalue_statistic': result['max_eigen_stat'],
                'cointegration_rank': result['rank'],
                'is_cointegrated': result['is_cointegrated'],
                'eigenvalues': result['eigenvalues'],
                'eigenvectors': result['eigenvectors'],
                'p_value_trace': result['p_value_trace'],
                'critical_value_trace': result['critical_value_trace']
            }
            
        except Exception as e:
            return self._error_result(str(e))
    
    def _johansen_test(self, data: np.ndarray, lags: int = 1) -> Dict:
        """
        Perform the Johansen cointegration test.
        
        Args:
            data: Array of price series (n_obs x n_vars)
            lags: Number of lags to include
            
        Returns:
            Dictionary with test results
        """
        n_obs, n_vars = data.shape
        
        if n_vars != 2:
            raise ValueError("Johansen test implemented for 2 variables only")
        
        # Create lagged differences for VECM
        diff_data = np.diff(data, axis=0)
        level_data = data[:-1]  # Lagged levels
        
        # If we have lags > 1, we'd need to create additional lagged differences
        # For simplicity, using lag=1
        
        # Estimate the VECM components
        # Delta Y_t = alpha * beta' * Y_{t-1} + Gamma * Delta Y_{t-1} + epsilon_t
        
        # For the two-step Engle-Granger approach embedded in Johansen
        y_diff = diff_data
        y_lag = level_data
        
        # Residuals from regressing differences on lagged differences
        if lags > 0 and len(diff_data) > lags:
            # Simple case: just use current differences
            r0 = y_diff
            r1 = y_lag
        else:
            r0 = y_diff
            r1 = y_lag
        
        # Calculate covariance matrices
        # S00 = Cov(r0, r0), S11 = Cov(r1, r1), S01 = Cov(r0, r1)
        s00 = np.cov(r0.T)
        s11 = np.cov(r1.T)
        s01 = np.cov(r0.T, r1.T)[:n_vars, n_vars:]
        s10 = s01.T
        
        # Solve the generalized eigenvalue problem
        # |S01 * S11^(-1) * S10 - lambda * S00| = 0
        try:
            s11_inv = np.linalg.inv(s11)
            matrix = np.dot(np.dot(s01, s11_inv), s10)
            s00_inv = np.linalg.inv(s00)
            
            # Generalized eigenvalue problem
            eigenvalues, eigenvectors = np.linalg.eig(
                np.dot(s00_inv, matrix)
            )
            
            # Sort eigenvalues in descending order
            idx = eigenvalues.argsort()[::-1]
            eigenvalues = eigenvalues[idx]
            eigenvectors = eigenvectors[:, idx]
            
            # Calculate test statistics
            trace_stat = -n_obs * np.sum(np.log(1 - eigenvalues))
            max_eigen_stat = -n_obs * np.log(1 - eigenvalues[0])
            
            # Determine cointegration rank
            is_cointegrated = trace_stat > self.critical_values['r=0']
            rank = 1 if is_cointegrated else 0
            
            # Calculate approximate p-value for trace statistic
            p_value_trace = 1 - chi2.cdf(trace_stat, df=n_vars)
            
            return {
                'trace_stat': float(trace_stat),
                'max_eigen_stat': float(max_eigen_stat),
                'rank': rank,
                'is_cointegrated': is_cointegrated,
                'eigenvalues': eigenvalues.tolist(),
                'eigenvectors': eigenvectors.tolist(),
                'p_value_trace': float(p_value_trace),
                'critical_value_trace': self.critical_values['r=0']
            }
            
        except np.linalg.LinAlgError as e:
            raise ValueError(f"Matrix computation failed: {e}")


def engle_granger_test(series1: pd.Series, series2: pd.Series) -> Dict:
    """
    Perform the Engle-Granger two-step cointegration test.
    
    Args:
        series1: First price series
        series2: Second price series
        
    Returns:
        Dictionary with test results
    """
    try:
        # Align data
        aligned_data = pd.DataFrame({
            'y1': series1,
            'y2': series2
        }).dropna()
        
        if len(aligned_data) < 30:
            return {'success': False, 'error': 'Insufficient data'}
        
        y1 = aligned_data['y1'].values
        y2 = aligned_data['y2'].values
        
        # Step 1: Estimate cointegrating regression
        # y1 = alpha + beta * y2 + residuals
        X = np.vstack([y2, np.ones(len(y2))]).T
        beta, alpha = np.linalg.lstsq(X, y1, rcond=None)[0]
        
        # Calculate residuals
        residuals = y1 - (alpha + beta * y2)
        
        # Step 2: Test for unit root in residuals using ADF test
        adf_result = augmented_dickey_fuller_test(residuals)
        
        # Critical values for Engle-Granger test (5% level, 2 variables)
        eg_critical_value = -3.34
        is_cointegrated = adf_result['adf_stat'] < eg_critical_value
        
        return {
            'success': True,
            'cointegrating_coefficient': float(beta),
            'intercept': float(alpha),
            'adf_statistic': adf_result['adf_stat'],
            'adf_p_value': adf_result['p_value'],
            'is_cointegrated': is_cointegrated,
            'critical_value': eg_critical_value,
            'residuals_mean': float(np.mean(residuals)),
            'residuals_std': float(np.std(residuals))
        }
        
    except Exception as e:
        return {'success': False, 'error': str(e)}


def augmented_dickey_fuller_test(series: np.ndarray, lags: int = 1) -> Dict:
    """
    Simple implementation of the Augmented Dickey-Fuller test.
    
    Args:
        series: Time series data
        lags: Number of lags to include
        
    Returns:
        Dictionary with ADF test results
    """
    try:
        # Prepare data for ADF regression
        # Delta y_t = alpha + beta*t + gamma*y_{t-1} + sum(delta_i * Delta y_{t-i}) + error
        
        y = series[lags:]
        y_lag1 = series[lags-1:-1]
        dy = np.diff(series)
        dy_current = dy[lags-1:]
        
        # Create lagged differences matrix
        X = [y_lag1, np.ones(len(y_lag1))]  # y_{t-1} and constant
        
        # Add lagged differences if lags > 1
        for i in range(1, lags):
            if lags-1-i >= 0:
                dy_lag = dy[lags-1-i:-i if i > 0 else None]
                if len(dy_lag) == len(dy_current):
                    X.append(dy_lag)
        
        X = np.column_stack(X)
        
        # OLS regression
        coeffs = np.linalg.lstsq(X, dy_current, rcond=None)[0]
        
        # The test statistic is the t-statistic for gamma (coefficient of y_{t-1})
        residuals = dy_current - X @ coeffs
        mse = np.mean(residuals**2)
        
        # Standard error of gamma coefficient
        xtx_inv = np.linalg.inv(X.T @ X)
        se_gamma = np.sqrt(mse * xtx_inv[0, 0])
        
        # t-statistic
        t_stat = coeffs[0] / se_gamma
        
        # Approximate p-value (simplified)
        p_value = 0.05 if t_stat < -2.5 else 0.10
        
        return {
            'adf_stat': float(t_stat),
            'p_value': float(p_value),
            'coefficients': coeffs.tolist(),
            'standard_errors': [float(se_gamma)] + [0.0] * (len(coeffs)-1)
        }
        
    except Exception as e:
        return {
            'adf_stat': 0.0,
            'p_value': 1.0,
            'error': str(e)
        }